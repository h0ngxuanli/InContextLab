{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/LeqiZhou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-12-05 09:34:38,422 - INFO - Initializing AMPLIFY framework\n",
      "2024-12-05 09:34:38,422 - INFO - Initializing ProxyModel with 2 labels on cpu\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-12-05 09:34:39,519 - INFO - Starting AMPLIFY pipeline\n",
      "2024-12-05 09:34:39,519 - INFO - Selecting samples using MCS\n",
      "Computing MCS scores: 100%|██████████| 5/5 [00:00<00:00, 67.55it/s]\n",
      "2024-12-05 09:34:39,595 - INFO - Generating visualization\n",
      "2024-12-05 09:34:39,651 - INFO - Plot saved to outputs/mcs_distribution.png\n",
      "2024-12-05 09:34:39,651 - INFO - Generating final prompt\n",
      "Generating explanations: 100%|██████████| 3/3 [00:00<00:00, 11.36it/s]\n",
      "2024-12-05 09:34:40,291 - ERROR - Error in main: create_token_visualization() missing 1 required positional argument: 'prompt_text'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_token_visualization() missing 1 required positional argument: 'prompt_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 510\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# For Jupyter notebook usage\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 510\u001b[0m     results \u001b[38;5;241m=\u001b[39m main()\n",
      "Cell \u001b[0;32mIn[2], line 486\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    479\u001b[0m prompt \u001b[38;5;241m=\u001b[39m framework\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    480\u001b[0m     validation_set\u001b[38;5;241m=\u001b[39mvalidation_data,\n\u001b[1;32m    481\u001b[0m     visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    482\u001b[0m     save_plot\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/mcs_distribution.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m )\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# Create and display interactive visualization\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m visualization \u001b[38;5;241m=\u001b[39m visualize_amplify_results(framework, validation_data)\n\u001b[1;32m    487\u001b[0m display(visualization)\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# Print raw results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 451\u001b[0m, in \u001b[0;36mvisualize_amplify_results\u001b[0;34m(framework, samples)\u001b[0m\n\u001b[1;32m    437\u001b[0m     token_scores \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    438\u001b[0m         {\n\u001b[1;32m    439\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m: token\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mĠ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens[:min_len])\n\u001b[1;32m    443\u001b[0m     ]\n\u001b[1;32m    445\u001b[0m     samples_with_scores\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text,\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: label,\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m: token_scores\n\u001b[1;32m    449\u001b[0m     })\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m create_token_visualization(samples_with_scores)\n",
      "\u001b[0;31mTypeError\u001b[0m: create_token_visualization() missing 1 required positional argument: 'prompt_text'"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Dict, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def clean_token(token: str) -> str:\n",
    "    \"\"\"Clean special tokens from GPT-2 tokenizer output.\"\"\"\n",
    "    # Remove 'Ġ' character that represents spaces\n",
    "    return token.replace('Ġ', '')\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    \"\"\"Class for keeping track of a sample with its metadata.\"\"\"\n",
    "    text: str\n",
    "    label: str\n",
    "    score: Optional[float] = None\n",
    "    explanation: Optional[List[str]] = None\n",
    "\n",
    "class ProxyModel:\n",
    "    \"\"\"GPT-2 based proxy model as used in the paper.\"\"\"\n",
    "    def __init__(self, num_labels: int, device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        logger.info(f\"Initializing ProxyModel with {num_labels} labels on {device}\")\n",
    "        self.device = device\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Initialize tokenizer and model\n",
    "        try:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            self.model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=num_labels)\n",
    "            \n",
    "            # Add padding token if not present\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                self.model.config.pad_token_id = self.tokenizer.eos_token_id\n",
    "            \n",
    "            self.model.to(device)\n",
    "            self.model.eval()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing model: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Generate prediction scores.\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            probs = F.softmax(outputs.logits, dim=-1)[0].cpu()\n",
    "            return {str(i): float(prob) for i, prob in enumerate(probs)}\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in prediction: {str(e)}\")\n",
    "            return {str(i): 0.0 for i in range(self.num_labels)}\n",
    "\n",
    "class ExplanationGenerator:\n",
    "    def __init__(self, proxy_model: ProxyModel):\n",
    "        self.proxy_model = proxy_model\n",
    "\n",
    "    def vanilla_gradients(self, text: str, label: str) -> np.ndarray:\n",
    "        \"\"\"Generate explanations using vanilla gradients.\"\"\"\n",
    "        try:\n",
    "            self.proxy_model.model.zero_grad()\n",
    "            \n",
    "            inputs = self.proxy_model.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(self.proxy_model.device)\n",
    "            \n",
    "            embeddings = self.proxy_model.model.transformer.wte(inputs[\"input_ids\"])\n",
    "            embeddings.retain_grad()\n",
    "            \n",
    "            outputs = self.proxy_model.model(inputs_embeds=embeddings)\n",
    "            target_class = int(label)\n",
    "            \n",
    "            # Just use gradients without multiplying by input\n",
    "            outputs.logits[0, target_class].backward()\n",
    "            \n",
    "            gradients = embeddings.grad\n",
    "            if gradients is None:\n",
    "                return np.zeros(len(inputs[\"input_ids\"][0]))\n",
    "            \n",
    "            # Take L2 norm of gradients\n",
    "            token_attributions = torch.norm(gradients[0], p=2, dim=-1).cpu().numpy()\n",
    "            tokens = [clean_token(token) for token in self.proxy_model.tokenizer.tokenize(text)]\n",
    "            token_attributions = [attr for attr, token in zip(token_attributions, tokens) if token and token.lower() not in stopwords.words('english')]\n",
    "            \n",
    "            return np.array(token_attributions)\n",
    "            \n",
    "            return token_attributions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in vanilla gradients: {str(e)}\")\n",
    "            return np.zeros(1)\n",
    "\n",
    "    def contrastive_gradients(self, text: str, label: str) -> np.ndarray:\n",
    "        \"\"\"Generate contrastive explanations.\"\"\"\n",
    "        try:\n",
    "            self.proxy_model.model.zero_grad()\n",
    "            \n",
    "            inputs = self.proxy_model.tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(self.proxy_model.device)\n",
    "            \n",
    "            # Get predictions first\n",
    "            with torch.no_grad():\n",
    "                outputs = self.proxy_model.model(**inputs)\n",
    "                predicted_class = outputs.logits.argmax(dim=-1).item()\n",
    "            \n",
    "            # Get gradients for true class\n",
    "            embeddings = self.proxy_model.model.transformer.wte(inputs[\"input_ids\"])\n",
    "            embeddings.retain_grad()\n",
    "            \n",
    "            outputs = self.proxy_model.model(inputs_embeds=embeddings)\n",
    "            target_class = int(label)\n",
    "            \n",
    "            # First backward pass with retain_graph=True\n",
    "            outputs.logits[0, target_class].backward(retain_graph=True)\n",
    "            true_gradients = embeddings.grad.clone()\n",
    "            \n",
    "            # Reset gradients\n",
    "            self.proxy_model.model.zero_grad()\n",
    "            embeddings.grad = None\n",
    "            \n",
    "            # Second backward pass for predicted class\n",
    "            outputs = self.proxy_model.model(inputs_embeds=embeddings)\n",
    "            outputs.logits[0, predicted_class].backward()\n",
    "            pred_gradients = embeddings.grad.clone()\n",
    "            \n",
    "            if true_gradients is None or pred_gradients is None:\n",
    "                return np.zeros(len(inputs[\"input_ids\"][0]))\n",
    "            \n",
    "            # Compute contrastive gradients\n",
    "            contrastive = true_gradients - pred_gradients\n",
    "            tokens = [clean_token(token) for token in self.proxy_model.tokenizer.tokenize(text)]\n",
    "            true_gradients = [attr for attr, token in zip(true_gradients[0], tokens) if token and token.lower() not in stopwords.words('english')]\n",
    "            pred_gradients = [attr for attr, token in zip(pred_gradients[0], tokens) if token and token.lower() not in stopwords.words('english')]\n",
    "            \n",
    "            contrastive = np.array(true_gradients) - np.array(pred_gradients)\n",
    "            return np.linalg.norm(contrastive, ord=2, axis=-1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in contrastive gradients: {str(e)}\")\n",
    "            return np.zeros(1)\n",
    "\n",
    "    def generate_explanation(self, text: str, label: str, method: str = \"vanilla\") -> np.ndarray:\n",
    "        \"\"\"Generate explanations using specified method.\"\"\"\n",
    "        if method == \"vanilla\":\n",
    "            return self.vanilla_gradients(text, label)\n",
    "        elif method == \"contrastive\":\n",
    "            return self.contrastive_gradients(text, label)\n",
    "        else:\n",
    "            logger.warning(f\"Unknown method {method}, falling back to vanilla gradients\")\n",
    "            return self.vanilla_gradients(text, label)\n",
    "\n",
    "class SampleSelector:\n",
    "    \"\"\"Selects samples based on Misclassification Confidence Score.\"\"\"\n",
    "    def __init__(self, proxy_model: ProxyModel, top_k: int):\n",
    "        self.proxy_model = proxy_model\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def compute_mcs(self, text: str, true_label: str) -> float:\n",
    "        \"\"\"Compute Misclassification Confidence Score.\"\"\"\n",
    "        try:\n",
    "            predictions = self.proxy_model.predict(text)\n",
    "            true_score = predictions.get(true_label, 0)\n",
    "            max_false_score = max(\n",
    "                (score for label, score in predictions.items() if label != true_label),\n",
    "                default=0\n",
    "            )\n",
    "            return max_false_score - true_score\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing MCS: {str(e)}\")\n",
    "            return 0.0\n",
    "\n",
    "    def select_samples(self, samples: List[Tuple[str, str]]) -> List[Sample]:\n",
    "        \"\"\"Select top-k samples based on MCS.\"\"\"\n",
    "        scored_samples = []\n",
    "        \n",
    "        for text, label in tqdm(samples, desc=\"Computing MCS scores\"):\n",
    "            try:\n",
    "                mcs = self.compute_mcs(text, label)\n",
    "                scored_samples.append(Sample(text=text, label=label, score=mcs))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing sample: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        scored_samples.sort(key=lambda x: x.score if x.score is not None else float('-inf'), reverse=True)\n",
    "        return scored_samples[:self.top_k]\n",
    "\n",
    "class AMPLIFY:\n",
    "    \"\"\"Main AMPLIFY framework implementation.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int,\n",
    "        top_k_samples: int = 3,\n",
    "        top_k_keywords: int = 5,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ):\n",
    "        logger.info(\"Initializing AMPLIFY framework\")\n",
    "        self.num_labels = num_labels\n",
    "        self.device = device\n",
    "        self.top_k_samples = top_k_samples\n",
    "        self.top_k_keywords = top_k_keywords\n",
    "        \n",
    "        # Initialize components\n",
    "        self.proxy_model = ProxyModel(num_labels=num_labels, device=device)\n",
    "        self.explanation_generator = ExplanationGenerator(self.proxy_model)\n",
    "        self.sample_selector = SampleSelector(self.proxy_model, top_k_samples)\n",
    "\n",
    "    def visualize_scores(self, samples: List[Sample], save_path: Optional[str] = None):\n",
    "        \"\"\"Visualize MCS distribution.\"\"\"\n",
    "        scores = [s.score for s in samples if s.score is not None]\n",
    "        if not scores:\n",
    "            logger.warning(\"No scores to visualize\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(scores, bins=20, color='blue', alpha=0.7)\n",
    "        plt.title(\"Misclassification Confidence Score Distribution\")\n",
    "        plt.xlabel(\"MCS\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            logger.info(f\"Plot saved to {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def generate_prompt(self, samples: List[Sample]) -> str:\n",
    "        \"\"\"Generate the complete prompt with explanations.\"\"\"\n",
    "        prompt_parts = []\n",
    "        \n",
    "        for sample in tqdm(samples, desc=\"Generating explanations\"):\n",
    "            try:\n",
    "                # Generate attributions\n",
    "                attributions = self.explanation_generator.generate_explanation(\n",
    "                    sample.text,\n",
    "                    sample.label\n",
    "                )\n",
    "                \n",
    "                # Get tokens and match lengths\n",
    "                tokens = self.proxy_model.tokenizer.tokenize(sample.text)\n",
    "                min_len = min(len(tokens), len(attributions))\n",
    "                tokens = tokens[:min_len]\n",
    "                attributions = attributions[:min_len]\n",
    "                \n",
    "                # Get top keywords and clean them\n",
    "                token_scores = list(zip(tokens, attributions))\n",
    "                token_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_keywords = [clean_token(token) for token, _ in token_scores[:self.top_k_keywords]]\n",
    "                \n",
    "                # Store explanation\n",
    "                sample.explanation = top_keywords\n",
    "                \n",
    "                # Format prompt\n",
    "                rationale = (\n",
    "                    f\"The key words: {', '.join(top_keywords)} are crucial clues \"\n",
    "                    f\"for predicting {sample.label} as the correct answer.\"\n",
    "                )\n",
    "                prompt_part = (\n",
    "                    f\"Input: {sample.text}\\n\"\n",
    "                    f\"Rationale: {rationale}\\n\"\n",
    "                    f\"Label: {sample.label}\\n\"\n",
    "                )\n",
    "                prompt_parts.append(prompt_part)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating prompt part: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "    def run(\n",
    "        self,\n",
    "        validation_set: List[Tuple[str, str]],\n",
    "        visualize: bool = True,\n",
    "        save_plot: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Execute the complete AMPLIFY pipeline.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting AMPLIFY pipeline\")\n",
    "            \n",
    "            # Step 1: Select samples\n",
    "            logger.info(\"Selecting samples using MCS\")\n",
    "            selected_samples = self.sample_selector.select_samples(validation_set)\n",
    "            \n",
    "            # Step 2: Visualize if requested\n",
    "            if visualize:\n",
    "                logger.info(\"Generating visualization\")\n",
    "                self.visualize_scores(selected_samples, save_plot)\n",
    "            \n",
    "            # Step 3: Generate prompt\n",
    "            logger.info(\"Generating final prompt\")\n",
    "            prompt = self.generate_prompt(selected_samples)\n",
    "            \n",
    "            return prompt\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in AMPLIFY pipeline: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import json\n",
    "\n",
    "def create_token_visualization(samples_with_scores):\n",
    "    \"\"\"Create an interactive HTML visualization for token scores.\"\"\"\n",
    "    \n",
    "    html_template = '''\n",
    "    <div style=\"font-family: Arial, sans-serif; max-width: 900px; margin: 20px auto;\">\n",
    "        <style>\n",
    "            .token-container {\n",
    "                margin: 10px 0;\n",
    "                padding: 15px;\n",
    "                border: 1px solid #ddd;\n",
    "                border-radius: 8px;\n",
    "            }\n",
    "            .token {\n",
    "                display: inline-block;\n",
    "                margin: 2px;\n",
    "                padding: 5px 10px;\n",
    "                border-radius: 4px;\n",
    "                position: relative;\n",
    "                cursor: pointer;\n",
    "            }\n",
    "            .token:hover .tooltip {\n",
    "                display: block;\n",
    "            }\n",
    "            .tooltip {\n",
    "                display: none;\n",
    "                position: absolute;\n",
    "                bottom: 100%;\n",
    "                left: 50%;\n",
    "                transform: translateX(-50%);\n",
    "                background-color: #333;\n",
    "                color: white;\n",
    "                padding: 4px 8px;\n",
    "                border-radius: 4px;\n",
    "                font-size: 12px;\n",
    "                white-space: nowrap;\n",
    "                z-index: 100;\n",
    "            }\n",
    "            .prompt-container {\n",
    "                margin-top: 20px;\n",
    "                padding: 15px;\n",
    "                background-color: #f8f9fa;\n",
    "                border-radius: 8px;\n",
    "            }\n",
    "            .section-title {\n",
    "                font-weight: bold;\n",
    "                margin: 10px 0;\n",
    "                color: #2c5282;\n",
    "            }\n",
    "        </style>\n",
    "        \n",
    "        <div class=\"section-title\">Token Explanation Score Visualization</div>\n",
    "        \n",
    "        <div id=\"visualization-container\">\n",
    "            {% for sample in samples %}\n",
    "            <div class=\"token-container\">\n",
    "                <div style=\"margin-bottom: 10px;\"><b>Sample {{ loop.index }}:</b> Label = {{ sample.label }}</div>\n",
    "                <div>\n",
    "                    {% for token in sample.token_scores %}\n",
    "                    <span class=\"token\" \n",
    "                          style=\"background-color: rgb({{ 255 - token.score * 255 }}, {{ 255 - token.score * 255 }}, 255);\">\n",
    "                        {{ token.token }}\n",
    "                        <span class=\"tooltip\">Score: {{ \"%.3f\"|format(token.score) }}</span>\n",
    "                    </span>\n",
    "                    {% endfor %}\n",
    "                </div>\n",
    "            </div>\n",
    "            {% endfor %}\n",
    "        </div>\n",
    "\n",
    "        <div class=\"section-title\">Generated Training Prompt</div>\n",
    "        <div class=\"prompt-container\">\n",
    "            <pre style=\"margin: 0; white-space: pre-wrap;\">{{ prompt }}</pre>\n",
    "        </div>\n",
    "    </div>\n",
    "    '''\n",
    "    \n",
    "    # Generate prompt\n",
    "    prompt_parts = []\n",
    "    for sample in samples_with_scores:\n",
    "        # Get top 5 tokens by score\n",
    "        top_tokens = sorted(sample['token_scores'], key=lambda x: x['score'], reverse=True)[:5]\n",
    "        top_token_texts = [t['token'] for t in top_tokens]\n",
    "        \n",
    "        prompt_part = (\n",
    "            f\"Input: {sample['text']}\\n\"\n",
    "            f\"Rationale: The key words: {', '.join(top_token_texts)} are crucial clues \"\n",
    "            f\"for predicting {sample['label']} as the correct answer.\\n\"\n",
    "            f\"Label: {sample['label']}\"\n",
    "        )\n",
    "        prompt_parts.append(prompt_part)\n",
    "    \n",
    "    final_prompt = \"\\n\\n\".join(prompt_parts)\n",
    "    \n",
    "    # Convert scores to 0-1 range if needed\n",
    "    for sample in samples_with_scores:\n",
    "        scores = [t['score'] for t in sample['token_scores']]\n",
    "        max_score = max(abs(min(scores)), abs(max(scores)))\n",
    "        for token in sample['token_scores']:\n",
    "            token['score'] = abs(token['score']) / max_score if max_score != 0 else 0\n",
    "\n",
    "    # Replace template variables\n",
    "    from jinja2 import Template\n",
    "    template = Template(html_template)\n",
    "    html_content = template.render(\n",
    "        samples=samples_with_scores,\n",
    "        prompt=final_prompt\n",
    "    )\n",
    "    \n",
    "    return HTML(html_content)\n",
    "\n",
    "# Example usage in notebook:\n",
    "def visualize_amplify_results(framework, samples):\n",
    "    \"\"\"Create visualization from AMPLIFY results.\"\"\"\n",
    "    samples_with_scores = []\n",
    "    \n",
    "    for text, label in samples:\n",
    "        # Get attributions\n",
    "        attributions = framework.explanation_generator.generate_explanation(text, label)\n",
    "        \n",
    "        # Get tokens\n",
    "        tokens = framework.proxy_model.tokenizer.tokenize(text)\n",
    "        min_len = min(len(tokens), len(attributions))\n",
    "        \n",
    "        # Create token scores\n",
    "        token_scores = [\n",
    "            {\n",
    "                \"token\": token.replace('Ġ', ''),\n",
    "                \"score\": float(attributions[i])\n",
    "            }\n",
    "            for i, token in enumerate(tokens[:min_len])\n",
    "        ]\n",
    "        \n",
    "        samples_with_scores.append({\n",
    "            \"text\": text,\n",
    "            \"label\": label,\n",
    "            \"token_scores\": token_scores\n",
    "        })\n",
    "    \n",
    "    return create_token_visualization(samples_with_scores)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of AMPLIFY framework with interactive visualization.\"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Example data (Snarks task from the paper)\n",
    "    validation_data = [\n",
    "        (\"Is this sarcastic? 'What a great day!' he said while getting soaked in the rain.\", \"1\"),\n",
    "        (\"Nice work showing up an hour late!\", \"1\"),\n",
    "        (\"The weather is beautiful today.\", \"0\"),\n",
    "        (\"I love waiting in long lines!\", \"1\"),\n",
    "        (\"This is the best day ever!\", \"1\"),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        # Initialize AMPLIFY\n",
    "        framework = AMPLIFY(\n",
    "            num_labels=2,  # Binary classification\n",
    "            top_k_samples=3,\n",
    "            top_k_keywords=3\n",
    "        )\n",
    "\n",
    "        # Generate prompt\n",
    "        prompt = framework.run(\n",
    "            validation_set=validation_data,\n",
    "            visualize=True,\n",
    "            save_plot=\"outputs/mcs_distribution.png\"\n",
    "        )\n",
    "\n",
    "        # Create and display interactive visualization\n",
    "        visualization = visualize_amplify_results(framework, validation_data)\n",
    "        display(visualization)\n",
    "\n",
    "        # Print raw results\n",
    "        print(\"\\nRaw Generated Prompt:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(prompt)\n",
    "\n",
    "        # Also save the prompt to a file\n",
    "        with open(\"outputs/generated_prompt.txt\", \"w\") as f:\n",
    "            f.write(prompt)\n",
    "        \n",
    "        return {\n",
    "            'framework': framework,\n",
    "            'prompt': prompt,\n",
    "            'validation_data': validation_data\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# For Jupyter notebook usage\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marketpulse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
