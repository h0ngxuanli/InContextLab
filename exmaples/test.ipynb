{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "class SemanticICLFramework:\n",
    "    def __init__(self, model_name, dataset_name):\n",
    "        \"\"\"\n",
    "        Initialize the framework with a model and dataset.\n",
    "\n",
    "        Parameters:\n",
    "            model_name (str): The Hugging Face model name.\n",
    "            dataset_name (str): The dataset name (from Hugging Face).\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load dataset\n",
    "        self.dataset = load_dataset(dataset_name)\n",
    "\n",
    "        # Preprocess dataset\n",
    "        self.processed_data = self.preprocess_dataset()\n",
    "\n",
    "    def preprocess_dataset(self):\n",
    "        \"\"\"\n",
    "        Preprocess the dataset by tokenizing and extracting triplets.\n",
    "\n",
    "        Returns:\n",
    "            dict: Preprocessed train and test datasets.\n",
    "        \"\"\"\n",
    "        processed = {}\n",
    "        for split in self.dataset:\n",
    "            processed[split] = [self.preprocess_example(example) for example in self.dataset[split]]\n",
    "        return processed\n",
    "\n",
    "    def preprocess_example(self, example):\n",
    "        \"\"\"\n",
    "        Preprocess a single example by tokenizing text and extracting triplets.\n",
    "\n",
    "        Parameters:\n",
    "            example (dict): A single example from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            dict: Preprocessed example with tokens and triplets.\n",
    "        \"\"\"\n",
    "        text = self.extract_text(example)\n",
    "        tokens = self.tokenizer(text, truncation=True, padding=True)\n",
    "        triplets = self.extract_triplets(example)\n",
    "        return {\"text\": text, \"tokens\": tokens, \"triplets\": triplets}\n",
    "\n",
    "    def extract_text(self, example):\n",
    "        \"\"\"\n",
    "        Extract text from the dataset example.\n",
    "\n",
    "        Parameters:\n",
    "            example (dict): A single example from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            str: Extracted text.\n",
    "        \"\"\"\n",
    "        if self.dataset_name == \"thunlp/few_rel\":\n",
    "            return \" \".join(example.get(\"tokens\", []))\n",
    "        elif self.dataset_name == \"Babelscape/rebel-dataset\":\n",
    "            return example.get(\"context\", \"\")\n",
    "        elif self.dataset_name == \"thu-coai/kd_conv_with_kb\":\n",
    "            return example.get(\"content\", \"\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported dataset for text extraction.\")\n",
    "\n",
    "    def extract_triplets(self, example):\n",
    "        \"\"\"\n",
    "        Extract triplets (head, relation, tail) from the example.\n",
    "\n",
    "        Parameters:\n",
    "            example (dict): A single example from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            list: List of triplets (head, relation, tail).\n",
    "        \"\"\"\n",
    "        if self.dataset_name == \"thunlp/few_rel\":\n",
    "            head = example.get(\"head\", {}).get(\"text\", \"\")\n",
    "            tail = example.get(\"tail\", {}).get(\"text\", \"\")\n",
    "            relation = example.get(\"relation\", \"\")\n",
    "            return [(head, relation, tail)]\n",
    "        elif self.dataset_name == \"Babelscape/rebel-dataset\": # x\n",
    "            return example.get(\"triplets\", [])\n",
    "        elif self.dataset_name == \"thu-coai/kd_conv_with_kb\": # x\n",
    "            head = example.get(\"name\", \"\")\n",
    "            relation = example.get(\"attrname\", \"\")\n",
    "            tail = example.get(\"attrvalue\", \"\")\n",
    "            return [(head, relation, tail)]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def analyze_attention_heads(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Analyze attention heads for Query-Key and Output-Value circuits across various model architectures.\n",
    "\n",
    "        Parameters:\n",
    "            input_ids (torch.Tensor): Tokenized input IDs.\n",
    "            attention_mask (torch.Tensor): Attention mask for padding.\n",
    "\n",
    "        Returns:\n",
    "            tuple: QK circuits, OV circuits, and raw attentions.\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "        attentions = outputs.attentions  # List of attention matrices from each layer\n",
    "\n",
    "        qk_circuits, ov_circuits = [], []\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.get_layers(self.model)):\n",
    "            if hasattr(layer, \"attn\") and hasattr(layer.attn, \"c_attn\"):  # GPT-like models\n",
    "                attn = layer.attn\n",
    "                qkv_proj = attn.c_attn.weight  # Combined QKV projections\n",
    "                q_proj = qkv_proj[:, :self.model.config.n_embd]\n",
    "                k_proj = qkv_proj[:, self.model.config.n_embd:2*self.model.config.n_embd]\n",
    "                v_proj = qkv_proj[:, 2*self.model.config.n_embd:]\n",
    "                o_proj = attn.c_proj.weight\n",
    "            elif hasattr(layer, \"attention\"):  # BERT-like models\n",
    "                attn = layer.attention.self\n",
    "                q_proj = attn.query.weight\n",
    "                k_proj = attn.key.weight\n",
    "                v_proj = attn.value.weight\n",
    "                o_proj = layer.attention.output.dense.weight\n",
    "            elif hasattr(layer, \"self_attn\"):  # T5/BART-like models\n",
    "                attn = layer.self_attn\n",
    "                q_proj = attn.q_proj.weight\n",
    "                k_proj = attn.k_proj.weight\n",
    "                v_proj = attn.v_proj.weight\n",
    "                o_proj = attn.out_proj.weight\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported layer structure at layer {layer_idx}\")\n",
    "\n",
    "            # Ensure dimensions match for matrix multiplication\n",
    "            if v_proj.shape[1] != o_proj.shape[0]:\n",
    "                raise ValueError(f\"Matrix dimension mismatch in layer {layer_idx}: v_proj {v_proj.shape}, o_proj {o_proj.shape}\")\n",
    "\n",
    "            # Compute QK and OV circuits\n",
    "            qk = q_proj @ k_proj.T\n",
    "            ov = v_proj @ o_proj.T\n",
    "\n",
    "            qk_circuits.append(qk)\n",
    "            ov_circuits.append(ov)\n",
    "\n",
    "        return qk_circuits, ov_circuits, attentions\n",
    "\n",
    "\n",
    "    def get_layers(self, model):\n",
    "        \"\"\"\n",
    "        Dynamically retrieve the layers of the model based on its architecture.\n",
    "\n",
    "        Parameters:\n",
    "            model (transformers.PreTrainedModel): The Hugging Face model.\n",
    "\n",
    "        Returns:\n",
    "            list: List of model layers.\n",
    "        \"\"\"\n",
    "        if hasattr(model, \"transformer\") and hasattr(model.transformer, \"h\"):  # GPT-like models\n",
    "            return model.transformer.h\n",
    "        elif hasattr(model, \"encoder\") and hasattr(model.encoder, \"layer\"):  # Encoder-decoder models\n",
    "            return model.encoder.layer\n",
    "        elif hasattr(model, \"encoder\") and hasattr(model.encoder, \"layers\"):  # Some T5 variants\n",
    "            return model.encoder.layers\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model architecture\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def compute_relation_index(self, attentions, triplets, input_ids):\n",
    "        \"\"\"\n",
    "        Compute relation indices for attention heads.\n",
    "\n",
    "        Parameters:\n",
    "            attentions: Raw attention weights from the model.\n",
    "            triplets: Extracted triplets (head, relation, tail).\n",
    "            input_ids: Tokenized input IDs for the input sequence.\n",
    "\n",
    "        Returns:\n",
    "            float: Average relation index for attention heads.\n",
    "        \"\"\"\n",
    "        relation_indices = []\n",
    "        token_to_idx = {self.tokenizer.decode([id_]): idx for idx, id_ in enumerate(input_ids[0])}\n",
    "\n",
    "        for head_attn in attentions:  # Shape: (num_heads, seq_len, seq_len)\n",
    "            for triplet in triplets:\n",
    "                head, _, tail = triplet\n",
    "                head_idx = token_to_idx.get(head, None)\n",
    "                tail_idx = token_to_idx.get(tail, None)\n",
    "\n",
    "                if head_idx is None or tail_idx is None:\n",
    "                    # Skip triplets where tokens are not found\n",
    "                    continue\n",
    "\n",
    "                # Compute the attention score between the head and tail tokens\n",
    "                relation_score = head_attn[:, head_idx, tail_idx].mean().item()\n",
    "                relation_indices.append(relation_score)\n",
    "\n",
    "        return np.mean(relation_indices) if relation_indices else 0.0\n",
    "\n",
    "\n",
    "    def monitor_icl(self, dataset, steps=100):\n",
    "        \"\"\"\n",
    "        Monitor In-Context Learning (ICL) abilities over time.\n",
    "\n",
    "        Parameters:\n",
    "            dataset: Preprocessed dataset.\n",
    "            steps (int): Number of steps to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            dict: Loss reduction, format compliance, and pattern discovery metrics.\n",
    "        \"\"\"\n",
    "        loss_reduction, format_compliance, pattern_discovery = [], [], []\n",
    "\n",
    "        for step, example in enumerate(dataset[:steps]):\n",
    "            tokens = self.tokenizer(example[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "            labels = tokens[\"input_ids\"]\n",
    "            outputs = self.model(**tokens, labels=labels)\n",
    "\n",
    "            # Loss reduction\n",
    "            loss = outputs.loss.item()\n",
    "            loss_reduction.append(loss)\n",
    "\n",
    "            # Format compliance (correct structure of output)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            compliance = (predictions == labels).float().mean().item()\n",
    "            format_compliance.append(compliance)\n",
    "\n",
    "            # Pattern discovery (accurate prediction)\n",
    "            correct_predictions = (predictions == labels).float().sum().item()\n",
    "            pattern_discovery.append(correct_predictions / len(labels))\n",
    "\n",
    "        return {\n",
    "            \"loss_reduction\": loss_reduction,\n",
    "            \"format_compliance\": format_compliance,\n",
    "            \"pattern_discovery\": pattern_discovery,\n",
    "        }\n",
    "\n",
    "    def correlate_attention_with_icl(self, relation_indices, icl_metrics):\n",
    "        \"\"\"\n",
    "        Compute correlation between attention head behaviors and ICL metrics.\n",
    "\n",
    "        Parameters:\n",
    "            relation_indices: Relation index values for attention heads.\n",
    "            icl_metrics: ICL metrics (e.g., loss reduction).\n",
    "\n",
    "        Returns:\n",
    "            float: Correlation coefficient.\n",
    "        \"\"\"\n",
    "        correlation = np.corrcoef(relation_indices, icl_metrics)\n",
    "        return correlation\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "framework = SemanticICLFramework(model_name=\"gpt2\", dataset_name=\"thunlp/few_rel\")\n",
    "processed_data = framework.processed_data[\"train\"]\n",
    "\n",
    "relation_indices = []\n",
    "for example in processed_data[:100]:\n",
    "    tokens = framework.tokenizer(example[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    input_ids, attention_mask = tokens[\"input_ids\"], tokens[\"attention_mask\"]\n",
    "    qk_circuits, ov_circuits, attentions = framework.analyze_attention_heads(input_ids, attention_mask)\n",
    "    \n",
    "    # Pass input_ids to map tokens to indices\n",
    "    relation_idx = framework.compute_relation_index(attentions, example[\"triplets\"], input_ids)\n",
    "    relation_indices.append(relation_idx)\n",
    "\n",
    "\n",
    "# Correlate with ICL\n",
    "icl_metrics = framework.monitor_icl(processed_data, steps=100)\n",
    "correlation = framework.correlate_attention_with_icl(relation_indices, icl_metrics[\"loss_reduction\"])\n",
    "print(f\"Correlation between relation indices and loss reduction: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSemanticICLFramework\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name, dataset_name):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "class SemanticICLFramework:\n",
    "    def __init__(self, model_name, dataset_name):\n",
    "        \"\"\"\n",
    "        Initialize the framework with a model and dataset.\n",
    "\n",
    "        Parameters:\n",
    "            model_name (str): The Hugging Face model name.\n",
    "            dataset_name (str): The dataset name (from Hugging Face).\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        # Load model and tokenizer\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load dataset\n",
    "        self.dataset = load_dataset(dataset_name)\n",
    "\n",
    "        # Preprocess dataset\n",
    "        self.processed_data = self.preprocess_dataset()\n",
    "\n",
    "    def preprocess_dataset(self):\n",
    "        \"\"\"\n",
    "        Preprocess the dataset by tokenizing and extracting triplets.\n",
    "        \"\"\"\n",
    "        processed = {}\n",
    "        for split in self.dataset:\n",
    "            processed[split] = [self.preprocess_example(example) for example in self.dataset[split]]\n",
    "        return processed\n",
    "\n",
    "    def preprocess_example(self, example):\n",
    "        \"\"\"\n",
    "        Preprocess a single example by tokenizing text and extracting triplets.\n",
    "        \"\"\"\n",
    "        text = self.extract_text(example)\n",
    "        tokens = self.tokenizer(text, truncation=True, padding=True)\n",
    "        triplets = self.extract_triplets(example)\n",
    "        return {\"text\": text, \"tokens\": tokens, \"triplets\": triplets}\n",
    "\n",
    "    def extract_text(self, example):\n",
    "        \"\"\"\n",
    "        Extract text from the dataset example.\n",
    "        \"\"\"\n",
    "        if self.dataset_name == \"thunlp/few_rel\":\n",
    "            return \" \".join(example.get(\"tokens\", []))\n",
    "        elif self.dataset_name == \"Babelscape/rebel-dataset\":\n",
    "            return example.get(\"context\", \"\")\n",
    "        elif self.dataset_name == \"thu-coai/kd_conv_with_kb\":\n",
    "            return example.get(\"content\", \"\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported dataset for text extraction.\")\n",
    "\n",
    "    def extract_triplets(self, example):\n",
    "        \"\"\"\n",
    "        Extract triplets (head, relation, tail) from the example.\n",
    "        \"\"\"\n",
    "        if self.dataset_name == \"thunlp/few_rel\":\n",
    "            head = example.get(\"head\", {}).get(\"text\", \"\")\n",
    "            tail = example.get(\"tail\", {}).get(\"text\", \"\")\n",
    "            relation = example.get(\"relation\", \"\")\n",
    "            return [(head, relation, tail)]\n",
    "        elif self.dataset_name == \"Babelscape/rebel-dataset\":\n",
    "            return example.get(\"triplets\", [])\n",
    "        elif self.dataset_name == \"thu-coai/kd_conv_with_kb\":\n",
    "            head = example.get(\"name\", \"\")\n",
    "            relation = example.get(\"attrname\", \"\")\n",
    "            tail = example.get(\"attrvalue\", \"\")\n",
    "            return [(head, relation, tail)]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def analyze_attention_heads(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Analyze attention heads for Query-Key and Output-Value circuits across various model architectures.\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "        attentions = outputs.attentions  # List of attention matrices from each layer\n",
    "        return attentions\n",
    "\n",
    "    def analyze_qk_ov_circuits(self, attentions):\n",
    "        \"\"\"\n",
    "        Analyze QK and OV circuits to understand token relationships and output transformations.\n",
    "        \"\"\"\n",
    "        qk_insights, ov_insights = {}, {}\n",
    "        for layer_idx, attn_layer in enumerate(attentions):\n",
    "            qk_matrix = np.mean(attn_layer[:, :, :, :], axis=0)  # Average over heads\n",
    "            ov_matrix = np.mean(attn_layer[:, :, :, :], axis=0)  # Average over heads\n",
    "\n",
    "            qk_insights[layer_idx] = qk_matrix\n",
    "            ov_insights[layer_idx] = ov_matrix\n",
    "\n",
    "        return {\"QK\": qk_insights, \"OV\": ov_insights}\n",
    "\n",
    "    def layer_head_analysis(self, attentions):\n",
    "        \"\"\"\n",
    "        Perform layer-wise and head-specific analysis of attention matrices.\n",
    "        \"\"\"\n",
    "        layer_head_patterns = {}\n",
    "        for layer_idx, layer_attn in enumerate(attentions):\n",
    "            head_patterns = []\n",
    "            for head_idx in range(layer_attn.shape[1]):  # Iterate over attention heads\n",
    "                avg_attn = layer_attn[:, head_idx, :, :].mean(axis=0)\n",
    "                head_patterns.append(avg_attn)\n",
    "\n",
    "            layer_head_patterns[layer_idx] = head_patterns\n",
    "        return layer_head_patterns\n",
    "\n",
    "    def attribute_attention(self, attentions, triplets, input_ids):\n",
    "        \"\"\"\n",
    "        Attribute attention scores to triplet components.\n",
    "        \"\"\"\n",
    "        attribution_scores = []\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "        for triplet in triplets:\n",
    "            head, relation, tail = triplet\n",
    "            head_idx = tokens.index(head) if head in tokens else None\n",
    "            tail_idx = tokens.index(tail) if tail in tokens else None\n",
    "\n",
    "            if head_idx is not None and tail_idx is not None:\n",
    "                avg_attn = np.mean([attn[:, head_idx, tail_idx].mean() for attn in attentions])\n",
    "                attribution_scores.append((triplet, avg_attn))\n",
    "\n",
    "        return attribution_scores\n",
    "\n",
    "    def visualize_layer_dynamics(self, layer_head_patterns):\n",
    "        \"\"\"\n",
    "        Visualize attention dynamics across layers and heads.\n",
    "        \"\"\"\n",
    "        for layer_idx, heads in layer_head_patterns.items():\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for head_idx, head_attn in enumerate(heads):\n",
    "                plt.plot(head_attn.mean(axis=0), label=f\"Head {head_idx}\")\n",
    "            plt.title(f\"Layer {layer_idx} Attention Dynamics\")\n",
    "            plt.xlabel(\"Tokens\")\n",
    "            plt.ylabel(\"Attention Score\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    def correlate_attention_with_metrics(self, relation_indices, icl_metrics):\n",
    "        \"\"\"\n",
    "        Compute and visualize correlation between attention and ICL metrics.\n",
    "        \"\"\"\n",
    "        correlation = np.corrcoef(relation_indices, icl_metrics[\"loss_reduction\"])[0, 1]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(relation_indices, icl_metrics[\"loss_reduction\"], alpha=0.7)\n",
    "        plt.title(\"Correlation Between Attention and Loss Reduction\")\n",
    "        plt.xlabel(\"Relation Index\")\n",
    "        plt.ylabel(\"Loss Reduction\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        return correlation\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "framework = SemanticICLFramework(model_name=\"gpt2\", dataset_name=\"Babelscape/rebel-dataset\")\n",
    "processed_data = framework.processed_data[\"train\"]\n",
    "\n",
    "relation_indices = []\n",
    "for example in processed_data[:5]:\n",
    "    tokens = framework.tokenizer(example[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    input_ids, attention_mask = tokens[\"input_ids\"], tokens[\"attention_mask\"]\n",
    "    attentions = framework.analyze_attention_heads(input_ids, attention_mask)\n",
    "\n",
    "    # Visualize attention for the first example\n",
    "    framework.visualize_layer_dynamics(framework.layer_head_analysis(attentions))\n",
    "\n",
    "    # Compute triplet attribution\n",
    "    attribution = framework.attribute_attention(attentions, example[\"triplets\"], input_ids)\n",
    "    print(f\"Attention Attribution: {attribution}\")\n",
    "\n",
    "# Example Correlation Visualization\n",
    "icl_metrics = {\"loss_reduction\": [0.1, 0.2, 0.15, 0.18, 0.12]}\n",
    "correlation = framework.correlate_attention_with_metrics(relation_indices, icl_metrics)\n",
    "print(f\"Correlation: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m901.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-18.1.0-cp312-cp312-macosx_12_0_arm64.whl (29.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, multiprocess, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.32.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.1.0 multiprocess-0.70.16 pyarrow-18.1.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
