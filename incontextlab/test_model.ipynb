{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating prompts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Prompts:\n",
      "==================================================\n",
      "\n",
      "Prompt 1:\n",
      "The key words 'Ġfantastic, Ġabsolutely, Ġamazing, Ġwas, and Ġmovie' are important clues to predict 'Positive' as the correct answer.\n",
      "\n",
      "Prompt 2:\n",
      "The key words 'Ġterrible, Ġno, Ġsense, Ġwas, and Ġacting' are important clues to predict 'Negative' as the correct answer.\n",
      "\n",
      "Prompt 3:\n",
      "The key words 'Ġgood, Ġwere, Ġneeded, Ġvisuals, and ,' are important clues to predict 'Neutral' as the correct answer.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    input_text: str\n",
    "    label: str\n",
    "    confidence_score: Optional[float] = None\n",
    "    mcs_score: Optional[float] = None\n",
    "    important_words: Optional[List[str]] = None\n",
    "\n",
    "class ProxyModel:\n",
    "    def __init__(self, model_name: str = \"cardiffnlp/twitter-roberta-base-sentiment\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Set up label mappings to match the model's labels\n",
    "        self.model.config.label2id = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "        self.model.config.id2label = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    \n",
    "    def compute_probabilities(self, sample: Sample) -> Tuple[float, float]:\n",
    "        if sample.label not in self.model.config.label2id:\n",
    "            raise ValueError(f\"Label '{sample.label}' not found in model's label2id mapping.\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            sample.input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=1)[0]\n",
    "            \n",
    "            label_idx = self.model.config.label2id[sample.label]\n",
    "            true_prob = probs[label_idx].item()\n",
    "            pred_prob = probs.max().item()\n",
    "        \n",
    "        return pred_prob, true_prob\n",
    "\n",
    "    def compute_mcs(self, sample: Sample) -> float:\n",
    "        pred_prob, true_prob = self.compute_probabilities(sample)\n",
    "        return pred_prob - true_prob\n",
    "\n",
    "    def get_attribution_scores(self, sample: Sample) -> Dict[str, float]:\n",
    "        if sample.label not in self.model.config.label2id:\n",
    "            raise ValueError(f\"Label '{sample.label}' not found in model's label2id mapping.\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            sample.input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        label_idx = self.model.config.label2id[sample.label]\n",
    "        \n",
    "        # Get embeddings and set requires_grad\n",
    "        embeddings = self.model.get_input_embeddings()(input_ids)\n",
    "        embeddings.requires_grad_()\n",
    "\n",
    "        # Use Integrated Gradients for attribution\n",
    "        ig = IntegratedGradients(self.forward_func)\n",
    "        \n",
    "        attributions, delta = ig.attribute(\n",
    "            embeddings,\n",
    "            baselines=torch.zeros_like(embeddings),\n",
    "            target=label_idx,\n",
    "            additional_forward_args=attention_mask,\n",
    "            return_convergence_delta=True\n",
    "        )\n",
    "        \n",
    "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        scores = attributions.detach().cpu().numpy()\n",
    "        \n",
    "        return dict(zip(tokens, scores))\n",
    "\n",
    "    def forward_func(self, embeddings, attention_mask):\n",
    "        outputs = self.model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "class SampleSelector:\n",
    "    def __init__(self, strategy: str = \"h-mcs\"):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def select_samples(self, samples: List[Sample], k: int, proxy_model: ProxyModel) -> List[Sample]:\n",
    "        if len(samples) < k:\n",
    "            logger.warning(\"Number of samples is less than k.\")\n",
    "            return samples\n",
    "        \n",
    "        # Compute MCS scores\n",
    "        for sample in samples:\n",
    "            if sample.mcs_score is None:\n",
    "                sample.mcs_score = proxy_model.compute_mcs(sample)\n",
    "        \n",
    "        if self.strategy == \"random\":\n",
    "            return list(np.random.choice(samples, k, replace=False))\n",
    "        elif self.strategy == \"h-mcs\":\n",
    "            return sorted(samples, key=lambda x: x.mcs_score, reverse=True)[:k]\n",
    "        else:\n",
    "            return sorted(samples, key=lambda x: x.mcs_score)[:k]\n",
    "\n",
    "class ExplanationGenerator:\n",
    "    def __init__(self, top_k: int = 5):\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def generate_explanation(self, sample: Sample, proxy_model: ProxyModel) -> List[str]:\n",
    "        scores = proxy_model.get_attribution_scores(sample)\n",
    "        \n",
    "        # Remove special tokens and clean token representations\n",
    "        filtered_scores = {}\n",
    "        for token, score in scores.items():\n",
    "            if token in proxy_model.tokenizer.all_special_tokens:\n",
    "                continue\n",
    "            clean_token = token.strip()\n",
    "            if clean_token:\n",
    "                filtered_scores[clean_token] = score\n",
    "        \n",
    "        # Sort words by absolute attribution score\n",
    "        sorted_words = sorted(filtered_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        important_words = []\n",
    "        for word, _ in sorted_words:\n",
    "            if word not in seen:\n",
    "                seen.add(word)\n",
    "                important_words.append(word)\n",
    "            if len(important_words) >= self.top_k:\n",
    "                break\n",
    "        return important_words\n",
    "\n",
    "class PromptGenerator:\n",
    "    def generate_prompt(self, sample: Sample) -> str:\n",
    "        if not sample.important_words:\n",
    "            return \"\"\n",
    "        \n",
    "        unique_words = list(dict.fromkeys(sample.important_words))  # Remove duplicates while preserving order\n",
    "        \n",
    "        if len(unique_words) == 1:\n",
    "            words_str = unique_words[0]\n",
    "        else:\n",
    "            words_str = \", \".join(unique_words[:-1]) + f\", and {unique_words[-1]}\"\n",
    "        return (f\"The key words '{words_str}' are important clues to predict \"\n",
    "                f\"'{sample.label}' as the correct answer.\")\n",
    "\n",
    "class AMPLIFY:\n",
    "    def __init__(self, proxy_model: ProxyModel, sample_selector: SampleSelector,\n",
    "                 explanation_generator: ExplanationGenerator, prompt_generator: PromptGenerator):\n",
    "        self.proxy_model = proxy_model\n",
    "        self.sample_selector = sample_selector\n",
    "        self.explanation_generator = explanation_generator\n",
    "        self.prompt_generator = prompt_generator\n",
    "\n",
    "    def process_samples(self, samples: List[Sample], k: int) -> List[str]:\n",
    "        # Select samples based on MCS scores\n",
    "        selected_samples = self.sample_selector.select_samples(samples, k, self.proxy_model)\n",
    "        \n",
    "        # Generate explanations and prompts\n",
    "        prompts = []\n",
    "        for sample in selected_samples:\n",
    "            sample.important_words = self.explanation_generator.generate_explanation(\n",
    "                sample, self.proxy_model)\n",
    "            prompt = self.prompt_generator.generate_prompt(sample)\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        return prompts\n",
    "\n",
    "def main():\n",
    "    # Initialize components\n",
    "    proxy_model = ProxyModel(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    sample_selector = SampleSelector(strategy=\"h-mcs\")\n",
    "    explanation_generator = ExplanationGenerator(top_k=5)\n",
    "    prompt_generator = PromptGenerator()\n",
    "    \n",
    "    # Create AMPLIFY instance\n",
    "    amplify = AMPLIFY(proxy_model, sample_selector, explanation_generator, prompt_generator)\n",
    "    \n",
    "    # Example samples\n",
    "    samples = [\n",
    "        Sample(\"The movie was absolutely fantastic, with amazing performances\", \"Positive\"),\n",
    "        Sample(\"The plot made no sense and the acting was terrible\", \"Negative\"),\n",
    "        Sample(\"While the visuals were good, the story needed work\", \"Neutral\"),\n",
    "        Sample(\"A masterpiece of modern cinema\", \"Positive\"),\n",
    "        Sample(\"Complete waste of time and money\", \"Negative\")\n",
    "    ]\n",
    "    \n",
    "    # Generate prompts\n",
    "    logger.info(\"Generating prompts...\")\n",
    "    prompts = amplify.process_samples(samples, k=3)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nGenerated Prompts:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\nPrompt {i}:\")\n",
    "        print(prompt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.12/site-packages/ninja-1.11.1.1-py3.12-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.12/site-packages/torchdrug-0.2.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.12/site-packages/fair_esm-2.0.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting captum\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from captum) (3.9.1.post1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from captum) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/anaconda3/lib/python3.12/site-packages (from captum) (2.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from captum) (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (2024.3.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: captum\n",
      "Successfully installed captum-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "@dataclass\n",
    "class Triplet:\n",
    "    head_token: str\n",
    "    relation: str\n",
    "    tail_token: str\n",
    "    head_pos: int\n",
    "    tail_pos: int\n",
    "\n",
    "class SemanticInductionAnalyzer:\n",
    "    def __init__(self, model: nn.Module, tokenizer, tau: float = 2.2):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tau = tau\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "        self.num_heads = model.config.num_attention_heads\n",
    "        self.num_layers = model.config.num_hidden_layers\n",
    "\n",
    "    def extract_attention_patterns(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract attention patterns from model's all layers and heads.\"\"\"\n",
    "        outputs = self.model(input_ids, output_attentions=True)\n",
    "        return torch.stack(outputs.attentions, dim=0)  # [layers, batch, heads, seq, seq]\n",
    "\n",
    "    def compute_relation_index(self, triplet: Triplet, input_ids: torch.Tensor) -> Dict[Tuple[int, int], float]:\n",
    "        \"\"\"Compute relation index for each attention head given a triplet.\"\"\"\n",
    "        attention_patterns = self.extract_attention_patterns(input_ids)\n",
    "        head_indices = {}\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            for head in range(self.num_heads):\n",
    "                # Get attention scores for current position\n",
    "                attn = attention_patterns[layer, 0, head]  # [seq_len, seq_len]\n",
    "                \n",
    "                # Check if head attends to head token strongly\n",
    "                if torch.argmax(attn[triplet.tail_pos]) == triplet.head_pos:\n",
    "                    attention_ratio = attn[triplet.tail_pos, triplet.head_pos] / \\\n",
    "                                    attn[triplet.tail_pos].max()\n",
    "                    \n",
    "                    if attention_ratio > self.tau:\n",
    "                        # Compute OV circuit influence\n",
    "                        logits = self._compute_output_value_influence(layer, head, triplet)\n",
    "                        head_indices[(layer, head)] = self._compute_index_from_logits(logits, triplet)\n",
    "\n",
    "        return head_indices\n",
    "\n",
    "    def _compute_output_value_influence(self, layer: int, head: int, triplet: Triplet) -> torch.Tensor:\n",
    "        \"\"\"Compute the influence of OV circuit on vocabulary.\"\"\"\n",
    "        Wv = self.model.layers[layer].attention.value\n",
    "        Wo = self.model.layers[layer].attention.output\n",
    "        \n",
    "        OV = torch.matmul(Wv[head], Wo[head])\n",
    "        return torch.matmul(OV, self.model.embeddings.weight.T)\n",
    "\n",
    "    def _compute_index_from_logits(self, logits: torch.Tensor, triplet: Triplet) -> float:\n",
    "        \"\"\"Compute relation index from logits.\"\"\"\n",
    "        tail_id = self.tokenizer.convert_tokens_to_ids(triplet.tail_token)\n",
    "        \n",
    "        # Normalize logits and get score for tail token\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        tail_prob = probs[tail_id].item()\n",
    "        \n",
    "        # Compare to mean probability\n",
    "        mean_prob = probs.mean().item()\n",
    "        return max(0, tail_prob - mean_prob)\n",
    "\n",
    "class RelationAnalysis:\n",
    "    def __init__(self, analyzer: SemanticInductionAnalyzer):\n",
    "        self.analyzer = analyzer\n",
    "        \n",
    "    def analyze_dependencies(self, text: str, dependencies: List[Tuple[str, str, str]]) -> Dict[str, Dict[Tuple[int, int], float]]:\n",
    "        \"\"\"Analyze syntactic dependencies in text.\"\"\"\n",
    "        input_ids = self.analyzer.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "        results = {}\n",
    "        \n",
    "        for head, rel, tail in dependencies:\n",
    "            triplet = self._create_triplet(text, head, rel, tail)\n",
    "            if triplet:\n",
    "                results[rel] = self.analyzer.compute_relation_index(triplet, input_ids)\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def _create_triplet(self, text: str, head: str, rel: str, tail: str) -> Triplet:\n",
    "        \"\"\"Create triplet with token positions.\"\"\"\n",
    "        tokens = self.analyzer.tokenizer.tokenize(text)\n",
    "        \n",
    "        # Find positions of head and tail tokens\n",
    "        head_pos = self._find_token_position(tokens, head)\n",
    "        tail_pos = self._find_token_position(tokens, tail)\n",
    "        \n",
    "        if head_pos is not None and tail_pos is not None:\n",
    "            return Triplet(head, rel, tail, head_pos, tail_pos)\n",
    "        return None\n",
    "    \n",
    "    def _find_token_position(self, tokens: List[str], target: str) -> int:\n",
    "        \"\"\"Find position of target token in sequence.\"\"\"\n",
    "        target_tokens = self.analyzer.tokenizer.tokenize(target)\n",
    "        for i in range(len(tokens) - len(target_tokens) + 1):\n",
    "            if tokens[i:i+len(target_tokens)] == target_tokens:\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    num_shots: int\n",
    "    batch_size: int \n",
    "    num_epochs: int\n",
    "    checkpoints: List[int]\n",
    "    tasks: List[str]\n",
    "\n",
    "class InContextLearningExperiments:\n",
    "    def __init__(self, model, tokenizer, config: ExperimentConfig):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        \n",
    "    def evaluate_format_compliance(self, checkpoint: int) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate format compliance for different tasks at checkpoint.\"\"\"\n",
    "        results = {}\n",
    "        for task in self.config.tasks:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for _ in range(self.config.num_epochs):\n",
    "                prompts = self._generate_prompts(task, self.config.num_shots)\n",
    "                outputs = self._get_model_outputs(prompts)\n",
    "                \n",
    "                for output in outputs:\n",
    "                    if self._check_format(output, task):\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                    \n",
    "            results[task] = correct / total\n",
    "        return results\n",
    "    \n",
    "    def evaluate_pattern_discovery(self, checkpoint: int) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate pattern discovery ability at checkpoint.\"\"\"\n",
    "        results = {}\n",
    "        for task in self.config.tasks:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for _ in range(self.config.num_epochs):\n",
    "                prompts = self._generate_prompts(task, self.config.num_shots)\n",
    "                outputs = self._get_model_outputs(prompts)\n",
    "                labels = self._get_labels(prompts)\n",
    "                \n",
    "                for output, label in zip(outputs, labels):\n",
    "                    if output == label:\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "                    \n",
    "            results[task] = correct / total\n",
    "        return results\n",
    "\n",
    "    def analyze_loss_reduction(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Analyze loss reduction over sequence length.\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for text in texts:\n",
    "            input_ids = self.tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "            outputs = self.model(input_ids, labels=input_ids)\n",
    "            \n",
    "            # Compute token-wise loss\n",
    "            loss = outputs.loss.view(-1).detach().numpy()\n",
    "            losses.append(loss)\n",
    "            \n",
    "        return np.mean(losses, axis=0)\n",
    "\n",
    "    def _generate_prompts(self, task: str, num_shots: int) -> List[str]:\n",
    "        \"\"\"Generate few-shot prompts for given task.\"\"\"\n",
    "        if task == \"binary_classification\":\n",
    "            examples = [\n",
    "                (\"apple\", \"January\", \"0\"),\n",
    "                (\"desk\", \"teacher\", \"1\"),\n",
    "                (\"orange\", \"March\", \"0\")\n",
    "            ]\n",
    "        elif task == \"relation_justification\":\n",
    "            examples = [\n",
    "                (\"cat\", \"chases\", \"mouse\", \"true\"),\n",
    "                (\"book\", \"reads\", \"student\", \"true\"),\n",
    "                (\"sky\", \"eats\", \"cloud\", \"false\")\n",
    "            ]\n",
    "            \n",
    "        # Select random examples for few-shot prompt\n",
    "        selected = np.random.choice(examples, num_shots, replace=False)\n",
    "        prompts = []\n",
    "        \n",
    "        for example in selected:\n",
    "            if task == \"binary_classification\":\n",
    "                prompts.append(f\"{example[0]}, {example[1]}: {example[2]}\")\n",
    "            elif task == \"relation_justification\":\n",
    "                prompts.append(f\"{example[0]}, {example[1]}, {example[2]}: {example[3]}\")\n",
    "                \n",
    "        return prompts\n",
    "\n",
    "    def _get_model_outputs(self, prompts: List[str]) -> List[str]:\n",
    "        \"\"\"Get model outputs for prompts.\"\"\"\n",
    "        outputs = []\n",
    "        for prompt in prompts:\n",
    "            input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            output = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=1,\n",
    "                pad_token_id=self.tokenizer.pad_token_id\n",
    "            )\n",
    "            outputs.append(self.tokenizer.decode(output[0][-1]))\n",
    "        return outputs\n",
    "\n",
    "class RelationVisualizer:\n",
    "    def __init__(self, num_layers: int, num_heads: int):\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "    def plot_relation_heatmap(self, relation_indices: Dict[Tuple[int, int], float], \n",
    "                            title: str, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot heatmap of relation indices across layers and heads.\"\"\"\n",
    "        matrix = np.zeros((self.num_layers, self.num_heads))\n",
    "        \n",
    "        for (layer, head), value in relation_indices.items():\n",
    "            matrix[layer, head] = value\n",
    "            \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(matrix, cmap='YlOrRd', \n",
    "                   xticklabels=range(self.num_heads),\n",
    "                   yticklabels=range(self.num_layers))\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.xlabel('Heads')\n",
    "        plt.ylabel('Layers')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        \n",
    "    def plot_training_progression(self, checkpoint_results: Dict[int, Dict[str, float]], \n",
    "                                metric: str, save_path: Optional[str] = None):\n",
    "        \"\"\"Plot progression of metrics across training checkpoints.\"\"\"\n",
    "        checkpoints = sorted(checkpoint_results.keys())\n",
    "        tasks = list(checkpoint_results[checkpoints[0]].keys())\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for task in tasks:\n",
    "            values = [checkpoint_results[cp][task] for cp in checkpoints]\n",
    "            plt.plot(checkpoints, values, label=task, marker='o')\n",
    "            \n",
    "        plt.title(f'{metric} Progression During Training')\n",
    "        plt.xlabel('Training Steps')\n",
    "        plt.ylabel(metric)\n",
    "        plt.legend()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, \n",
    "                 analyzer: SemanticInductionAnalyzer,\n",
    "                 experimenter: InContextLearningExperiments,\n",
    "                 visualizer: RelationVisualizer):\n",
    "        self.analyzer = analyzer\n",
    "        self.experimenter = experimenter\n",
    "        self.visualizer = visualizer\n",
    "        \n",
    "    def run_complete_analysis(self, texts: List[str], dependencies: List[Tuple[str, str, str]]):\n",
    "        \"\"\"Run complete analysis pipeline.\"\"\"\n",
    "        # Analyze semantic induction heads\n",
    "        relation_results = {}\n",
    "        for text, deps in zip(texts, dependencies):\n",
    "            analysis = RelationAnalysis(self.analyzer)\n",
    "            results = analysis.analyze_dependencies(text, deps)\n",
    "            relation_results.update(results)\n",
    "            \n",
    "        # Evaluate in-context learning\n",
    "        format_results = {}\n",
    "        pattern_results = {}\n",
    "        for checkpoint in self.experimenter.config.checkpoints:\n",
    "            format_results[checkpoint] = self.experimenter.evaluate_format_compliance(checkpoint)\n",
    "            pattern_results[checkpoint] = self.experimenter.evaluate_pattern_discovery(checkpoint)\n",
    "            \n",
    "        # Generate visualizations\n",
    "        self.visualizer.plot_relation_heatmap(relation_results, \"Semantic Relation Indices\")\n",
    "        self.visualizer.plot_training_progression(format_results, \"Format Compliance\")\n",
    "        self.visualizer.plot_training_progression(pattern_results, \"Pattern Discovery\")\n",
    "        \n",
    "        return {\n",
    "            'relation_results': relation_results,\n",
    "            'format_results': format_results,\n",
    "            'pattern_results': pattern_results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_format_compliance_experiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 92\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Example usage scenarios\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Scenario 1: Quick format compliance check\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m format_results \u001b[38;5;241m=\u001b[39m run_format_compliance_experiment()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Scenario 2: Pattern discovery analysis\u001b[39;00m\n\u001b[1;32m     95\u001b[0m pattern_results \u001b[38;5;241m=\u001b[39m run_pattern_discovery_experiment()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_format_compliance_experiment' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def main():\n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"internlm/internlm2-1.8b\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Configure experiments\n",
    "    config = ExperimentConfig(\n",
    "        num_shots=5,\n",
    "        batch_size=16,\n",
    "        num_epochs=3,\n",
    "        checkpoints=[200, 400, 800, 1600, 3200],\n",
    "        tasks=[\"binary_classification\", \"relation_justification\"]\n",
    "    )\n",
    "\n",
    "    # Initialize analysis components\n",
    "    analyzer = SemanticInductionAnalyzer(model, tokenizer)\n",
    "    experimenter = InContextLearningExperiments(model, tokenizer, config)\n",
    "    visualizer = RelationVisualizer(\n",
    "        num_layers=model.config.num_hidden_layers,\n",
    "        num_heads=model.config.num_attention_heads\n",
    "    )\n",
    "    runner = ExperimentRunner(analyzer, experimenter, visualizer)\n",
    "\n",
    "    # Example text and dependencies for analysis\n",
    "    texts = [\n",
    "        \"The cat chases the mouse in the garden\",\n",
    "        \"We present a CRF model for Event Detection\"\n",
    "    ]\n",
    "    \n",
    "    dependencies = [\n",
    "        [(\"cat\", \"subj\", \"chases\"), (\"mouse\", \"obj\", \"chases\")],\n",
    "        [(\"model\", \"obj\", \"present\"), (\"CRF\", \"mod\", \"model\")]\n",
    "    ]\n",
    "\n",
    "    # Run specific experiments\n",
    "    def run_format_compliance_experiment():\n",
    "        print(\"Running format compliance experiment...\")\n",
    "        results = experimenter.evaluate_format_compliance(checkpoint=1600)\n",
    "        print(\"Format compliance results:\", results)\n",
    "        \n",
    "        # Visualize results\n",
    "        visualizer.plot_relation_heatmap(\n",
    "            {(2, 3): 0.8, (3, 1): 0.6, (4, 2): 0.7}, \n",
    "            \"Format Compliance Heatmap\",\n",
    "            \"format_compliance.png\"\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def run_pattern_discovery_experiment():\n",
    "        print(\"Running pattern discovery experiment...\")\n",
    "        # Example text for testing pattern discovery\n",
    "        test_examples = [\n",
    "            \"apple, January: \",  # Expected: 0\n",
    "            \"desk, teacher: \",   # Expected: 1\n",
    "        ]\n",
    "        \n",
    "        results = experimenter.evaluate_pattern_discovery(checkpoint=3200)\n",
    "        print(\"Pattern discovery results:\", results)\n",
    "        return results\n",
    "\n",
    "    def analyze_semantic_relations():\n",
    "        print(\"Analyzing semantic relations...\")\n",
    "        relation_analysis = RelationAnalysis(analyzer)\n",
    "        \n",
    "        for text, deps in zip(texts, dependencies):\n",
    "            results = relation_analysis.analyze_dependencies(text, deps)\n",
    "            print(f\"\\nResults for text: {text}\")\n",
    "            for rel, indices in results.items():\n",
    "                print(f\"Relation {rel}:\", indices)\n",
    "\n",
    "    # Run complete analysis pipeline\n",
    "    def run_full_analysis():\n",
    "        print(\"Running complete analysis...\")\n",
    "        results = runner.run_complete_analysis(texts, dependencies)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nAnalysis Summary:\")\n",
    "        print(\"Number of relations analyzed:\", len(results['relation_results']))\n",
    "        print(\"Checkpoints evaluated:\", len(results['format_results']))\n",
    "        print(\"Tasks analyzed:\", len(results['pattern_results']))\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "    # Example usage scenarios\n",
    "\n",
    "    # Scenario 1: Quick format compliance check\n",
    "    format_results = run_format_compliance_experiment()\n",
    "\n",
    "    # Scenario 2: Pattern discovery analysis\n",
    "    pattern_results = run_pattern_discovery_experiment()\n",
    "\n",
    "    # Scenario 3: Semantic relation analysis\n",
    "    analyze_semantic_relations()\n",
    "\n",
    "    # Scenario 4: Complete analysis pipeline\n",
    "    full_results = run_full_analysis()\n",
    "\n",
    "    print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
