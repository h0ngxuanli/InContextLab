{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating prompts...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Prompts:\n",
      "==================================================\n",
      "\n",
      "Prompt 1:\n",
      "The key words 'Ġfantastic, Ġabsolutely, Ġamazing, Ġwas, and Ġmovie' are important clues to predict 'Positive' as the correct answer.\n",
      "\n",
      "Prompt 2:\n",
      "The key words 'Ġterrible, Ġno, Ġsense, Ġwas, and Ġacting' are important clues to predict 'Negative' as the correct answer.\n",
      "\n",
      "Prompt 3:\n",
      "The key words 'Ġgood, Ġwere, Ġneeded, Ġvisuals, and ,' are important clues to predict 'Neutral' as the correct answer.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class Sample:\n",
    "    input_text: str\n",
    "    label: str\n",
    "    confidence_score: Optional[float] = None\n",
    "    mcs_score: Optional[float] = None\n",
    "    important_words: Optional[List[str]] = None\n",
    "\n",
    "class ProxyModel:\n",
    "    def __init__(self, model_name: str = \"cardiffnlp/twitter-roberta-base-sentiment\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Set up label mappings to match the model's labels\n",
    "        self.model.config.label2id = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "        self.model.config.id2label = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    \n",
    "    def compute_probabilities(self, sample: Sample) -> Tuple[float, float]:\n",
    "        if sample.label not in self.model.config.label2id:\n",
    "            raise ValueError(f\"Label '{sample.label}' not found in model's label2id mapping.\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            sample.input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = F.softmax(logits, dim=1)[0]\n",
    "            \n",
    "            label_idx = self.model.config.label2id[sample.label]\n",
    "            true_prob = probs[label_idx].item()\n",
    "            pred_prob = probs.max().item()\n",
    "        \n",
    "        return pred_prob, true_prob\n",
    "\n",
    "    def compute_mcs(self, sample: Sample) -> float:\n",
    "        pred_prob, true_prob = self.compute_probabilities(sample)\n",
    "        return pred_prob - true_prob\n",
    "\n",
    "    def get_attribution_scores(self, sample: Sample) -> Dict[str, float]:\n",
    "        if sample.label not in self.model.config.label2id:\n",
    "            raise ValueError(f\"Label '{sample.label}' not found in model's label2id mapping.\")\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            sample.input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        label_idx = self.model.config.label2id[sample.label]\n",
    "        \n",
    "        # Get embeddings and set requires_grad\n",
    "        embeddings = self.model.get_input_embeddings()(input_ids)\n",
    "        embeddings.requires_grad_()\n",
    "\n",
    "        # Use Integrated Gradients for attribution\n",
    "        ig = IntegratedGradients(self.forward_func)\n",
    "        \n",
    "        attributions, delta = ig.attribute(\n",
    "            embeddings,\n",
    "            baselines=torch.zeros_like(embeddings),\n",
    "            target=label_idx,\n",
    "            additional_forward_args=attention_mask,\n",
    "            return_convergence_delta=True\n",
    "        )\n",
    "        \n",
    "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        scores = attributions.detach().cpu().numpy()\n",
    "        \n",
    "        return dict(zip(tokens, scores))\n",
    "\n",
    "    def forward_func(self, embeddings, attention_mask):\n",
    "        outputs = self.model(inputs_embeds=embeddings, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "class SampleSelector:\n",
    "    def __init__(self, strategy: str = \"h-mcs\"):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def select_samples(self, samples: List[Sample], k: int, proxy_model: ProxyModel) -> List[Sample]:\n",
    "        if len(samples) < k:\n",
    "            logger.warning(\"Number of samples is less than k.\")\n",
    "            return samples\n",
    "        \n",
    "        # Compute MCS scores\n",
    "        for sample in samples:\n",
    "            if sample.mcs_score is None:\n",
    "                sample.mcs_score = proxy_model.compute_mcs(sample)\n",
    "        \n",
    "        if self.strategy == \"random\":\n",
    "            return list(np.random.choice(samples, k, replace=False))\n",
    "        elif self.strategy == \"h-mcs\":\n",
    "            return sorted(samples, key=lambda x: x.mcs_score, reverse=True)[:k]\n",
    "        else:\n",
    "            return sorted(samples, key=lambda x: x.mcs_score)[:k]\n",
    "\n",
    "class ExplanationGenerator:\n",
    "    def __init__(self, top_k: int = 5):\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def generate_explanation(self, sample: Sample, proxy_model: ProxyModel) -> List[str]:\n",
    "        scores = proxy_model.get_attribution_scores(sample)\n",
    "        \n",
    "        # Remove special tokens and clean token representations\n",
    "        filtered_scores = {}\n",
    "        for token, score in scores.items():\n",
    "            if token in proxy_model.tokenizer.all_special_tokens:\n",
    "                continue\n",
    "            clean_token = token.strip()\n",
    "            if clean_token:\n",
    "                filtered_scores[clean_token] = score\n",
    "        \n",
    "        # Sort words by absolute attribution score\n",
    "        sorted_words = sorted(filtered_scores.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        important_words = []\n",
    "        for word, _ in sorted_words:\n",
    "            if word not in seen:\n",
    "                seen.add(word)\n",
    "                important_words.append(word)\n",
    "            if len(important_words) >= self.top_k:\n",
    "                break\n",
    "        return important_words\n",
    "\n",
    "class PromptGenerator:\n",
    "    def generate_prompt(self, sample: Sample) -> str:\n",
    "        if not sample.important_words:\n",
    "            return \"\"\n",
    "        \n",
    "        unique_words = list(dict.fromkeys(sample.important_words))  # Remove duplicates while preserving order\n",
    "        \n",
    "        if len(unique_words) == 1:\n",
    "            words_str = unique_words[0]\n",
    "        else:\n",
    "            words_str = \", \".join(unique_words[:-1]) + f\", and {unique_words[-1]}\"\n",
    "        return (f\"The key words '{words_str}' are important clues to predict \"\n",
    "                f\"'{sample.label}' as the correct answer.\")\n",
    "\n",
    "class AMPLIFY:\n",
    "    def __init__(self, proxy_model: ProxyModel, sample_selector: SampleSelector,\n",
    "                 explanation_generator: ExplanationGenerator, prompt_generator: PromptGenerator):\n",
    "        self.proxy_model = proxy_model\n",
    "        self.sample_selector = sample_selector\n",
    "        self.explanation_generator = explanation_generator\n",
    "        self.prompt_generator = prompt_generator\n",
    "\n",
    "    def process_samples(self, samples: List[Sample], k: int) -> List[str]:\n",
    "        # Select samples based on MCS scores\n",
    "        selected_samples = self.sample_selector.select_samples(samples, k, self.proxy_model)\n",
    "        \n",
    "        # Generate explanations and prompts\n",
    "        prompts = []\n",
    "        for sample in selected_samples:\n",
    "            sample.important_words = self.explanation_generator.generate_explanation(\n",
    "                sample, self.proxy_model)\n",
    "            prompt = self.prompt_generator.generate_prompt(sample)\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        return prompts\n",
    "\n",
    "def main():\n",
    "    # Initialize components\n",
    "    proxy_model = ProxyModel(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "    sample_selector = SampleSelector(strategy=\"h-mcs\")\n",
    "    explanation_generator = ExplanationGenerator(top_k=5)\n",
    "    prompt_generator = PromptGenerator()\n",
    "    \n",
    "    # Create AMPLIFY instance\n",
    "    amplify = AMPLIFY(proxy_model, sample_selector, explanation_generator, prompt_generator)\n",
    "    \n",
    "    # Example samples\n",
    "    samples = [\n",
    "        Sample(\"The movie was absolutely fantastic, with amazing performances\", \"Positive\"),\n",
    "        Sample(\"The plot made no sense and the acting was terrible\", \"Negative\"),\n",
    "        Sample(\"While the visuals were good, the story needed work\", \"Neutral\"),\n",
    "        Sample(\"A masterpiece of modern cinema\", \"Positive\"),\n",
    "        Sample(\"Complete waste of time and money\", \"Negative\")\n",
    "    ]\n",
    "    \n",
    "    # Generate prompts\n",
    "    logger.info(\"Generating prompts...\")\n",
    "    prompts = amplify.process_samples(samples, k=3)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nGenerated Prompts:\")\n",
    "    print(\"=\" * 50)\n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\nPrompt {i}:\")\n",
    "        print(prompt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.12/site-packages/ninja-1.11.1.1-py3.12-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.12/site-packages/torchdrug-0.2.1-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.12/site-packages/fair_esm-2.0.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting captum\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from captum) (3.9.1.post1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from captum) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.6 in /opt/anaconda3/lib/python3.12/site-packages (from captum) (2.2.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from captum) (4.66.4)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.6->captum) (2024.3.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->captum) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: captum\n",
      "Successfully installed captum-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
